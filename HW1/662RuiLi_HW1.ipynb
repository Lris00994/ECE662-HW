{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQuekH9aZ9Iw",
        "outputId": "a452c8f2-fcc5-4880-e2a8-720d9fd5124f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM losses per sample: [  0.      71.483   64.9102 158.7325]\n",
            "SVM average loss: 73.781425\n",
            "Softmax losses per sample: [9.54791801e-14 4.43306000e+01 6.09785000e+01 1.03038400e+02]\n",
            "Softmax average loss: 52.086875001172096\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "W = np.array([\n",
        "    [-0.57,  1.24, -3.37,  6.43],\n",
        "    [-5.53, -1.13, -8.05,  3.21],\n",
        "    [ 4.23,  0.98, -2.53, -7.67],\n",
        "    [-2.31, -1.84,  6.93, -8.66]\n",
        "])\n",
        "\n",
        "X = np.array([\n",
        "    [1.52, 2.63, 5.37, 4.94],  # cat\n",
        "    [8.87, 1.25, 4.49, 0.12],  # cat\n",
        "    [3.22, 4.63, 3.55, 5.41],  # dog\n",
        "    [1.38, 0.63, 2.90, 8.52]   # horse\n",
        "])\n",
        "y = np.array([0, 0, 1, 3])\n",
        "\n",
        "def svm_loss(W, X, y):\n",
        "    scores = X.dot(W.T)  # (N x 4)\n",
        "    N = X.shape[0]\n",
        "    losses = []\n",
        "    for i in range(N):\n",
        "        s = scores[i]\n",
        "        correct = s[y[i]]\n",
        "        margins = np.maximum(0, s - correct + 1)\n",
        "        margins[y[i]] = 0\n",
        "        losses.append(np.sum(margins))\n",
        "    return np.array(losses), np.mean(losses)\n",
        "\n",
        "# Softmax cross-entropy loss\n",
        "def softmax_loss(W, X, y):\n",
        "    scores = X.dot(W.T)\n",
        "    N = X.shape[0]\n",
        "    losses = []\n",
        "    for i in range(N):\n",
        "        s = scores[i]\n",
        "        s_shift = s - np.max(s)\n",
        "        exp_s = np.exp(s_shift)\n",
        "        probs = exp_s / np.sum(exp_s)\n",
        "        losses.append(-np.log(probs[y[i]]))\n",
        "    return np.array(losses), np.mean(losses)\n",
        "\n",
        "svm_losses, svm_avg = svm_loss(W, X, y)\n",
        "softmax_losses, softmax_avg = softmax_loss(W, X, y)\n",
        "\n",
        "print(\"SVM losses per sample:\", svm_losses)\n",
        "print(\"SVM average loss:\", svm_avg)\n",
        "print(\"Softmax losses per sample:\", softmax_losses)\n",
        "print(\"Softmax average loss:\", softmax_avg)\n"
      ]
    }
  ]
}